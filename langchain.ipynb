{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_t2ZECmk-bLr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Quick Start"
      ],
      "metadata": {
        "id": "Z4FYzW5D5lbX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISPBmkg25d2b",
        "outputId": "5f6c86e5-87c6-4436-e9c1-b6d4adae801e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.27 (from langchain-openai)\n",
            "  Downloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<2.0.0,>=1.10.0 (from langchain-openai)\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain-openai)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (3.7.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.27->langchain-openai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-core<0.2.0,>=0.1.27->langchain-openai)\n",
            "  Downloading langsmith-0.1.23-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (8.2.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain-openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain-openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.27->langchain-openai)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.27->langchain-openai)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.27->langchain-openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.0.7)\n",
            "Installing collected packages: orjson, jsonpointer, h11, tiktoken, jsonpatch, httpcore, langsmith, httpx, openai, langchain-core, langchain-openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-core-0.1.30 langchain-openai-0.0.8 langsmith-0.1.23 openai-1.13.3 orjson-3.9.15 tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm_openai = ChatOpenAI(openai_api_key=\"sk-XXXX\", model='gpt-3.5-turbo')"
      ],
      "metadata": {
        "id": "jlDWL92T5xsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_openai)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLQ3CMX150UJ",
        "outputId": "a6d9f11c-c7ee-4d6d-d224-aaec9ed25b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=<openai.resources.chat.completions.Completions object at 0x79c553b65b40> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79c553b66fe0> openai_api_key=SecretStr('**********') openai_proxy=''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_openai.invoke(\"What is space?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYZVFzQ952Kn",
        "outputId": "e02fb3a9-9f52-4952-dc09-e1e66f82d669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Space is the vast expanse that exists beyond the Earth's atmosphere, where objects such as planets, stars, galaxies, and other celestial bodies exist. It is a near-perfect vacuum with little to no matter, allowing for the movement of objects and the transmission of light and other forms of energy. Space is also the setting for various phenomena such as black holes, supernovae, and cosmic radiation. It is infinite in size and continues to expand as the universe evolves.\")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_openai.invoke(\"Can you tell me the revenue of my company in FY 2022-23\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnqJ7sIV65rp",
        "outputId": "20727b8d-98c7-417d-8e9e-f7ded11812fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but as an AI assistant, I do not have access to real-time or specific financial data. I recommend checking your company's financial reports, speaking with your finance department, or consulting with a financial advisor for accurate information on your company's revenue in FY 2022-23.\")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging face models"
      ],
      "metadata": {
        "id": "W4unt39P9Gcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTZCwOhW9KCf",
        "outputId": "885da8b5-58cf-4bfc-88fe-e4ba0cae8dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.11-py3-none-any.whl (807 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Collecting langchain-community<0.1,>=0.0.25 (from langchain)\n",
            "  Downloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.2,>=0.1.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.30)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.23)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 langchain-0.1.11 langchain-community-0.0.27 langchain-text-splitters-0.0.1 marshmallow-3.21.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_XXXX\"\n",
        "llm_hf = HuggingFaceHub(repo_id = \"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.1, \"max_length\":512})\n"
      ],
      "metadata": {
        "id": "R496pdmm-aHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365a8d00-0a20-4419-da02-cf77ed143c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_hf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74wabQDo_JGE",
        "outputId": "b527a236-1aa2-4989-f228-0d83c3c91fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': 'text2text-generation', 'model_kwargs': {'temperature': 0.1, 'max_length': 512}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_hf.invoke(\"Tell about space\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SRz-ouQh_St7",
        "outputId": "c4d097dd-7e47-4fb7-9571-8893e4009f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The space shuttle is a large spacecraft that is launched into space.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_hf.invoke(\"Can you tell me the revenue of my company in FY 2022-23\")"
      ],
      "metadata": {
        "id": "XH5qmR8q_lVI",
        "outputId": "5a42515b-95a3-4d67-b0f5-1ac82a4a826b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am a manager in a company.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompts\n"
      ],
      "metadata": {
        "id": "rRFaZvGbzdR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "WPSWfmfyzfoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are an expert in teaching science having more than 10 years of expereience.\n",
        "    You will be given a topic and you have to explain the topic in three different stages - child, young, adult.\n",
        "    Here child needs to be explained in very easy words, young and old people can also understand some tough words.\n",
        "\n",
        "    Topic: {topic}\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "a4eqg6r20an-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.format(topic=\"Explain about ozone layer\")\n",
        "prompt.format()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "gfJpxz493w2K",
        "outputId": "e9f404c3-a694-4c68-fa1c-a3560b990111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    You are an expert in teaching science having more than 10 years of expereience.\\n    You will be given a topic and you have to explain the topic in three different stages - child, young, adult.\\n    Here child needs to be explained in very easy words, young and old people can also understand some tough words.\\n\\n    Topic: Explain about ozone layer\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_openai.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5khIYHA32PQ",
        "outputId": "d963f3ae-c4f1-444d-c69d-c8e59e20d230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Child: The ozone layer is like a big invisible umbrella that protects us from the sun's harmful rays. It's like wearing sunscreen for the Earth!\\n\\nYoung: The ozone layer is a layer of gas in the Earth's atmosphere that helps to shield us from the sun's ultraviolet (UV) rays. It acts like a protective barrier, absorbing and filtering out the dangerous UV radiation before it reaches the Earth's surface.\\n\\nAdult: The ozone layer is primarily located in the stratosphere, about 10 to 30 kilometers above the Earth's surface. It is made up of ozone molecules (O3) which absorb and scatter the UV radiation, preventing it from reaching the Earth's surface where it can cause harm to living organisms. However, human activities such as the use of certain chemicals called chlorofluorocarbons (CFCs) have been damaging the ozone layer, leading to the formation of the ozone hole and increased UV radiation reaching the Earth. Efforts have been made to reduce the use of these harmful chemicals through international agreements like the Montreal Protocol, in order to protect and preserve the ozone layer for future generations.\")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_hf.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-e1MPQQL5VcJ",
        "outputId": "da4b5eaf-da0a-4a1f-d120-be9f8f6789b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The ozone layer is a layer of the atmosphere that protects the earth from harmful ultraviolet rays.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contextual Prompts"
      ],
      "metadata": {
        "id": "_t2ZECmk-bLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_openai.invoke(\"What was the revenue of comapny in fy 2023\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H0i1fVf-KFc",
        "outputId": "dac89e53-cf26-4ff1-fce0-fd23ac0b1a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but I do not have access to real-time data or specific company information. Please check the company's official financial reports or website for the most up-to-date information on their revenue for FY 2023.\")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contextual_prompt_temp = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Context: The fictional company, XYZ Corp, has seen steady revenue growth over the past nine fiscal years, from FY 2015 to FY 2023. In FY 2015, the company reported revenue of $100 million, which increased to $110 million in FY 2016. The growth trajectory continued, with revenues reaching $120 million in FY 2017, $130 million in FY 2018, and $140 million in FY 2019. Despite economic challenges, the company managed to achieve revenue of $150 million in FY 2020, demonstrating resilience and adaptability. The positive trend persisted in FY 2021, with revenue increasing to $160 million, followed by $170 million in FY 2022, and finally reaching $180 million in FY 2023. This consistent growth reflects XYZ Corp's strong market position and effective business strategies.\n",
        "\n",
        "    You are a chatbot and you are given data about a company.\n",
        "    You have to answer the following question in a friendly and professional tone.\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "_e0nYkQE567b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contextual_prompt = contextual_prompt_temp.format(question=\"What was the revenue of comapny in fy 2023\")\n",
        "contextual_prompt.format()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "wdcaeZH_9yGZ",
        "outputId": "798569b1-4cf3-4f1b-c64c-093e7feb4da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n    Context: The fictional company, XYZ Corp, has seen steady revenue growth over the past nine fiscal years, from FY 2015 to FY 2023. In FY 2015, the company reported revenue of $100 million, which increased to $110 million in FY 2016. The growth trajectory continued, with revenues reaching $120 million in FY 2017, $130 million in FY 2018, and $140 million in FY 2019. Despite economic challenges, the company managed to achieve revenue of $150 million in FY 2020, demonstrating resilience and adaptability. The positive trend persisted in FY 2021, with revenue increasing to $160 million, followed by $170 million in FY 2022, and finally reaching $180 million in FY 2023. This consistent growth reflects XYZ Corp's strong market position and effective business strategies.\\n    \\n    You are a chatbot and you are given data about a company. \\n    You are to answer the following question in a friendly and professional tone.\\n\\n    Question: What was the revenue of comapny in fy 2023\\n    \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_openai.invoke(contextual_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXR3il5W-FSH",
        "outputId": "a65fee0a-297f-473d-9bef-06142fba4d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The revenue of XYZ Corp in FY 2023 was $180 million. This marked a continued growth trajectory for the company over the past nine fiscal years. If you have any more questions or need further information, feel free to ask!')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chains"
      ],
      "metadata": {
        "id": "mAd2AFW0DM5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "P10A6ACmE-OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_prompt_template = \"\"\"\n",
        "    You are expert in classifying the input into categories.\n",
        "    There are 3 categories Politics, Sports, or Technology. You have to classify the input into any one of the category.\n",
        "\n",
        "    Here are some details about each category:\n",
        "    Politics: Input should fall into this category if it pertains to government, policies, or political events. Examples include news about elections, legislation, political campaigns, and government actions.\n",
        "\n",
        "    Sports: Input should fall into this category if it relates to sports, athletic events, or sports teams. Examples include news about game results, player trades, upcoming matches, and sports-related achievements.\n",
        "\n",
        "    Technology: Input should fall into this category if it involves advancements, innovations, or developments in technology. Examples include news about new gadgets, software releases, scientific discoveries, and technological breakthroughs.\n",
        "\n",
        "    Input: {input_text}\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "4pffI82uDPKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_prompt = PromptTemplate(\n",
        "    template=classification_prompt_template,\n",
        "    input_variables=[\"input_text\"],\n",
        ")"
      ],
      "metadata": {
        "id": "TSADpGJXFWUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llmchain = LLMChain(llm=llm_openai, prompt=classification_prompt, verbose=True)\n",
        "llmchain.invoke(\"New Bill Proposed to Reform Tax Policies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCIcYOEgDX-W",
        "outputId": "f1584fa6-2da9-44be-a87f-df92f8bf8f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "    You are expert in classifying the input into categories.\n",
            "    There are 3 categories Politics, Sports, or Technology. You have to classify the input into any one of the category.\n",
            "\n",
            "    Here are some details about each category:\n",
            "    Politics: Input should fall into this category if it pertains to government, policies, or political events. Examples include news about elections, legislation, political campaigns, and government actions.\n",
            "\n",
            "    Sports: Input should fall into this category if it relates to sports, athletic events, or sports teams. Examples include news about game results, player trades, upcoming matches, and sports-related achievements.\n",
            "\n",
            "    Technology: Input should fall into this category if it involves advancements, innovations, or developments in technology. Examples include news about new gadgets, software releases, scientific discoveries, and technological breakthroughs.\n",
            "\n",
            "    Input: New Bill Proposed to Reform Tax Policies\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_text': 'New Bill Proposed to Reform Tax Policies',\n",
              " 'text': 'Category: Politics'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PAL Chain\n"
      ],
      "metadata": {
        "id": "CMjPdOX5Hdy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_experimental\n",
        "from langchain_experimental.pal_chain.base import PALChain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd87xm7kHguc",
        "outputId": "a40a5ff4-aa47-4260-e4f4-1cd79c254a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.0.53-py3-none-any.whl (173 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/173.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m163.8/173.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.7/173.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain<0.2.0,>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.1.11)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.27 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.1.30)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.0.27)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.1.23)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_experimental) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_experimental) (23.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_experimental) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_experimental) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_experimental) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.8->langchain_experimental) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.8->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.8->langchain_experimental) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.8->langchain_experimental) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.8->langchain_experimental) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.8->langchain_experimental) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.8->langchain_experimental) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.8->langchain_experimental) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.8->langchain_experimental) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.8->langchain_experimental) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.8->langchain_experimental) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.0.0)\n",
            "Installing collected packages: langchain_experimental\n",
            "Successfully installed langchain_experimental-0.0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plc = PALChain.from_math_prompt(llm=llm_openai, verbose=True)\n",
        "plc.run(\"Add 2 and 3 and muliply it with 100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "h7-Bgg_0I4Fn",
        "outputId": "bf3407b9-b1cc-4ff1-f437-d8b4cd11f3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mdef solution():\n",
            "    \"\"\"Add 2 and 3 and multiply it with 100\"\"\"\n",
            "    result = (2 + 3) * 100\n",
            "    return result\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'500'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(plc.get_prompts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D25Z6kPKF6B",
        "outputId": "9a4ed762-2227-4a7c-bfa9-bf43c1da76b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Runnable.get_prompts of PALChain(verbose=True, llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\"\\n    money_initial = 23\\n    bagels = 5\\n    bagel_cost = 3\\n    money_spent = bagels * bagel_cost\\n    money_left = money_initial - money_spent\\n    result = money_left\\n    return result\\n\\n\\n\\n\\n\\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    \"\"\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\"\"\"\\n    golf_balls_initial = 58\\n    golf_balls_lost_tuesday = 23\\n    golf_balls_lost_wednesday = 2\\n    golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday\\n    result = golf_balls_left\\n    return result\\n\\n\\n\\n\\n\\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    \"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\"\"\\n    computers_initial = 9\\n    computers_per_day = 5\\n    num_days = 4  # 4 days between monday and thursday\\n    computers_added = computers_per_day * num_days\\n    computers_total = computers_initial + computers_added\\n    result = computers_total\\n    return result\\n\\n\\n\\n\\n\\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    \"\"\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\"\"\"\\n    toys_initial = 5\\n    mom_toys = 2\\n    dad_toys = 2\\n    total_received = mom_toys + dad_toys\\n    total_toys = toys_initial + total_received\\n    result = total_toys\\n    return result\\n\\n\\n\\n\\n\\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    \"\"\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\"\"\"\\n    jason_lollipops_initial = 20\\n    jason_lollipops_after = 12\\n    denny_lollipops = jason_lollipops_initial - jason_lollipops_after\\n    result = denny_lollipops\\n    return result\\n\\n\\n\\n\\n\\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    \"\"\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\"\"\"\\n    leah_chocolates = 32\\n    sister_chocolates = 42\\n    total_chocolates = leah_chocolates + sister_chocolates\\n    chocolates_eaten = 35\\n    chocolates_left = total_chocolates - chocolates_eaten\\n    result = chocolates_left\\n    return result\\n\\n\\n\\n\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    \"\"\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\"\"\"\\n    cars_initial = 3\\n    cars_arrived = 2\\n    total_cars = cars_initial + cars_arrived\\n    result = total_cars\\n    return result\\n\\n\\n\\n\\n\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    \"\"\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\"\"\"\\n    trees_initial = 15\\n    trees_after = 21\\n    trees_added = trees_after - trees_initial\\n    result = trees_added\\n    return result\\n\\n\\n\\n\\n\\nQ: {question}\\n\\n# solution in Python:\\n\\n\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ac136d6c880>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ac136d6dd20>, openai_api_key=SecretStr('**********'), openai_proxy='')), code_validations=<langchain_experimental.pal_chain.base.PALValidation object at 0x7ac133b9af20>)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SequentialChain"
      ],
      "metadata": {
        "id": "Ms6MsbTjLOEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classification_prompt_template_1 = \"\"\"\n",
        "    You are expert in classifying the input into categories.\n",
        "    There are 3 categories Politics, Sports, or Technology. You have to classify the input into any one of the category.\n",
        "\n",
        "    Input: {input_text_1}\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "IrWb_26aLP1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_prompt_1 = PromptTemplate(\n",
        "    template=classification_prompt_template_1,\n",
        "    input_variables=[\"input_text_1\"],\n",
        ")"
      ],
      "metadata": {
        "id": "WiBNs6XALqSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llmchain_for_classification = LLMChain(llm=llm_hf, prompt=classification_prompt_1, verbose=True)"
      ],
      "metadata": {
        "id": "i1bQGyBgL2Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joke_gen_prompt_template = \"\"\"\n",
        "    You are expert in creating rap song from a text.\n",
        "    You will be given an input. You have create a rap song from that word.\n",
        "\n",
        "    Input: {input_text_2}\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "HQoM9I2nL9xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joke_gen_prompt = PromptTemplate(\n",
        "    template=joke_gen_prompt_template,\n",
        "    input_variables=[\"input_text_2\"],\n",
        ")"
      ],
      "metadata": {
        "id": "zeEL7aGPMfkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llmchain_for_joke = LLMChain(llm=llm_openai, prompt=joke_gen_prompt, verbose=True)"
      ],
      "metadata": {
        "id": "4-6PsBPkMq39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "overall_chain = SimpleSequentialChain(\n",
        "                  chains=[llmchain_for_classification, llmchain_for_joke],\n",
        "                  verbose=True)"
      ],
      "metadata": {
        "id": "mrsIB620NPbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain.invoke(\"New Bill Proposed to Reform Tax Policies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI0P1DY0NdRb",
        "outputId": "10f0cffe-ab82-4ef7-bf4f-b5caa7388f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "    You are expert in classifying the input into categories.\n",
            "    There are 3 categories Politics, Sports, or Technology. You have to classify the input into any one of the category.\n",
            "\n",
            "    Input: New Bill Proposed to Reform Tax Policies\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mPolitics\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "    You are expert in creating rap song from a text.\n",
            "    You will be given an input. You have create a rap song from that word.\n",
            "\n",
            "    Input: Politics\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m(Verse 1)\n",
            "Yo, I'm spitting facts about politics\n",
            "It's a game of power, full of tricks\n",
            "Politicians talking, but do they really care?\n",
            "Or are they just in it for the power they wear?\n",
            "\n",
            "(Chorus)\n",
            "Politics, it's a dirty game\n",
            "Full of lies, deceit, and shame\n",
            "But we gotta stay informed, gotta stay aware\n",
            "So we can make a change, show we really care\n",
            "\n",
            "(Verse 2)\n",
            "Lobbyists, corruption, it's all the same\n",
            "Trying to push their agenda, playing the game\n",
            "But we the people, we hold the power\n",
            "We gotta stand up, make our voices louder\n",
            "\n",
            "(Chorus)\n",
            "Politics, it's a dirty game\n",
            "Full of lies, deceit, and shame\n",
            "But we gotta stay informed, gotta stay aware\n",
            "So we can make a change, show we really care\n",
            "\n",
            "(Bridge)\n",
            "Don't let them fool you, don't let them win\n",
            "We gotta fight back, let the truth begin\n",
            "We gotta rise up, make our voices heard\n",
            "Together we can make a change, spread the word\n",
            "\n",
            "(Chorus)\n",
            "Politics, it's a dirty game\n",
            "Full of lies, deceit, and shame\n",
            "But we gotta stay informed, gotta stay aware\n",
            "So we can make a change, show we really care\n",
            "\n",
            "(Outro)\n",
            "So let's come together, stand up tall\n",
            "And show the world that we won't fall\n",
            "Politics may be dirty, but we can clean it up\n",
            "Let's make a difference, let's rise above.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'New Bill Proposed to Reform Tax Policies',\n",
              " 'output': \"(Verse 1)\\nYo, I'm spitting facts about politics\\nIt's a game of power, full of tricks\\nPoliticians talking, but do they really care?\\nOr are they just in it for the power they wear?\\n\\n(Chorus)\\nPolitics, it's a dirty game\\nFull of lies, deceit, and shame\\nBut we gotta stay informed, gotta stay aware\\nSo we can make a change, show we really care\\n\\n(Verse 2)\\nLobbyists, corruption, it's all the same\\nTrying to push their agenda, playing the game\\nBut we the people, we hold the power\\nWe gotta stand up, make our voices louder\\n\\n(Chorus)\\nPolitics, it's a dirty game\\nFull of lies, deceit, and shame\\nBut we gotta stay informed, gotta stay aware\\nSo we can make a change, show we really care\\n\\n(Bridge)\\nDon't let them fool you, don't let them win\\nWe gotta fight back, let the truth begin\\nWe gotta rise up, make our voices heard\\nTogether we can make a change, spread the word\\n\\n(Chorus)\\nPolitics, it's a dirty game\\nFull of lies, deceit, and shame\\nBut we gotta stay informed, gotta stay aware\\nSo we can make a change, show we really care\\n\\n(Outro)\\nSo let's come together, stand up tall\\nAnd show the world that we won't fall\\nPolitics may be dirty, but we can clean it up\\nLet's make a difference, let's rise above.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agents"
      ],
      "metadata": {
        "id": "IvMp4EgmTIh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results\n",
        "%pip install --upgrade --quiet  youtube_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sfnkGxbXAjn",
        "outputId": "a5930316-528b-4518-e41a-6fb11c3dca1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "from langchain_community.tools import YouTubeSearchTool\n",
        "\n",
        "from langchain.agents import initialize_agent, load_tools, AgentType\n",
        "from langchain.tools import Tool"
      ],
      "metadata": {
        "id": "SZJqyiIXUNoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = \"XXXX\""
      ],
      "metadata": {
        "id": "NXpvUiBjWUFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = SerpAPIWrapper() #google_search\n",
        "yt_tool = YouTubeSearchTool() #youtube_link"
      ],
      "metadata": {
        "id": "hxpTwiISVGIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rap_gen_prompt_template = \"\"\"\n",
        "    You are expert in creating rap song from a text.\n",
        "    You will be given an input. You have create a rap song from that word.\n",
        "\n",
        "    Input: {input_text_2}\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "_1l8QEVcaCbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rap_gen_prompt = PromptTemplate(\n",
        "    template=rap_gen_prompt_template,\n",
        "    input_variables=[\"input_text_2\"],\n",
        ")"
      ],
      "metadata": {
        "id": "9IVgSRIyaORU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llmchain_for_rap = LLMChain(llm=llm_openai, prompt=rap_gen_prompt, verbose=True)"
      ],
      "metadata": {
        "id": "gBR_0xaLeHf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "\t    name=\"Search\",\n",
        "\t    func=search.run,\n",
        "\t    description=\"useful for when you need to perform a google search and answer some questions.\"\n",
        "\t),\n",
        "\tTool(\n",
        "\t    name=\"YT_video\",\n",
        "\t    func=yt_tool.run,\n",
        "\t    description=\"useful when you need to find youtube video link of any channel.\"\n",
        "\t),\n",
        "\tTool(\n",
        "\t    func=llmchain_for_rap.invoke,\n",
        "\t    name=\"rapGenerator\",\n",
        "\t    description=\"useful when you create a rap from some text.\"\n",
        "\t),\n",
        "]\n",
        "agent = initialize_agent(tools, llm_openai, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "pOj7nFs4ZEe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"Can you make a rap w=using word laptop\"\n",
        "agent.run(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Myj8n0dBduOi",
        "outputId": "684fcc7c-9706-4465-a151-ace3b4da52a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `rapGenerator` with `laptop`\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "    You are expert in creating rap song from a text.\n",
            "    You will be given an input. You have create a rap song from that word.\n",
            "\n",
            "    Input: laptop\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[38;5;200m\u001b[1;3m{'input_text_2': 'laptop', 'text': \"(Verse 1)\\nYo, I'm typing on my laptop, feeling so fly\\nGot my fingers dancing, feeling so high\\nFrom work to play, this device never stops\\nIt's my trusty companion, never gonna flop\\n\\n(Chorus)\\nLaptop, laptop, you're my favorite tool\\nHelping me stay connected, never acting a fool\\nFrom emails to music, you do it all\\nLaptop, oh laptop, you never let me fall\\n\\n(Verse 2)\\nI carry you with me wherever I go\\nFrom coffee shops to planes, you steal the show\\nWith your sleek design and powerful speed\\nYou're the one thing I know I'll always need\\n\\n(Chorus)\\nLaptop, laptop, you're my favorite tool\\nHelping me stay connected, never acting a fool\\nFrom emails to music, you do it all\\nLaptop, oh laptop, you never let me fall\\n\\n(Bridge)\\nYou're more than just a machine\\nYou're a part of my routine\\nFrom work to play, day to night\\nYou're always there, shining so bright\\n\\n(Chorus)\\nLaptop, laptop, you're my favorite tool\\nHelping me stay connected, never acting a fool\\nFrom emails to music, you do it all\\nLaptop, oh laptop, you never let me fall\\n\\n(Outro)\\nSo here's to you, my trusty friend\\nWith you by my side, I know I'll never bend\\nLaptop, oh laptop, you're the best\\nYou passed the test, now let's take a rest\"}\u001b[0m\u001b[32;1m\u001b[1;3mHere's a rap using the word \"laptop\":\n",
            "\n",
            "(Verse 1)\n",
            "Yo, I'm typing on my laptop, feeling so fly\n",
            "Got my fingers dancing, feeling so high\n",
            "From work to play, this device never stops\n",
            "It's my trusty companion, never gonna flop\n",
            "\n",
            "(Chorus)\n",
            "Laptop, laptop, you're my favorite tool\n",
            "Helping me stay connected, never acting a fool\n",
            "From emails to music, you do it all\n",
            "Laptop, oh laptop, you never let me fall\n",
            "\n",
            "(Verse 2)\n",
            "I carry you with me wherever I go\n",
            "From coffee shops to planes, you steal the show\n",
            "With your sleek design and powerful speed\n",
            "You're the one thing I know I'll always need\n",
            "\n",
            "(Chorus)\n",
            "Laptop, laptop, you're my favorite tool\n",
            "Helping me stay connected, never acting a fool\n",
            "From emails to music, you do it all\n",
            "Laptop, oh laptop, you never let me fall\n",
            "\n",
            "(Bridge)\n",
            "You're more than just a machine\n",
            "You're a part of my routine\n",
            "From work to play, day to night\n",
            "You're always there, shining so bright\n",
            "\n",
            "(Chorus)\n",
            "Laptop, laptop, you're my favorite tool\n",
            "Helping me stay connected, never acting a fool\n",
            "From emails to music, you do it all\n",
            "Laptop, oh laptop, you never let me fall\n",
            "\n",
            "(Outro)\n",
            "So here's to you, my trusty friend\n",
            "With you by my side, I know I'll never bend\n",
            "Laptop, oh laptop, you're the best\n",
            "You passed the test, now let's take a rest\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here\\'s a rap using the word \"laptop\":\\n\\n(Verse 1)\\nYo, I\\'m typing on my laptop, feeling so fly\\nGot my fingers dancing, feeling so high\\nFrom work to play, this device never stops\\nIt\\'s my trusty companion, never gonna flop\\n\\n(Chorus)\\nLaptop, laptop, you\\'re my favorite tool\\nHelping me stay connected, never acting a fool\\nFrom emails to music, you do it all\\nLaptop, oh laptop, you never let me fall\\n\\n(Verse 2)\\nI carry you with me wherever I go\\nFrom coffee shops to planes, you steal the show\\nWith your sleek design and powerful speed\\nYou\\'re the one thing I know I\\'ll always need\\n\\n(Chorus)\\nLaptop, laptop, you\\'re my favorite tool\\nHelping me stay connected, never acting a fool\\nFrom emails to music, you do it all\\nLaptop, oh laptop, you never let me fall\\n\\n(Bridge)\\nYou\\'re more than just a machine\\nYou\\'re a part of my routine\\nFrom work to play, day to night\\nYou\\'re always there, shining so bright\\n\\n(Chorus)\\nLaptop, laptop, you\\'re my favorite tool\\nHelping me stay connected, never acting a fool\\nFrom emails to music, you do it all\\nLaptop, oh laptop, you never let me fall\\n\\n(Outro)\\nSo here\\'s to you, my trusty friend\\nWith you by my side, I know I\\'ll never bend\\nLaptop, oh laptop, you\\'re the best\\nYou passed the test, now let\\'s take a rest'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Can you give me youtube video link of channel named MrBeast\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "IIQxYr5Dg3iZ",
        "outputId": "0a26a7a1-4a8c-44ae-8620-a67939950824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `YT_video` with `MrBeast`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m['https://www.youtube.com/watch?v=jaRfBM7ESfc&pp=ygUHTXJCZWFzdA%3D%3D', 'https://www.youtube.com/watch?v=krsBRQbOPQ4&pp=ygUHTXJCZWFzdA%3D%3D']\u001b[0m\u001b[32;1m\u001b[1;3mHere are the YouTube video links of the channel named MrBeast:\n",
            "1. [Video 1](https://www.youtube.com/watch?v=jaRfBM7ESfc&pp=ygUHTXJCZWFzdA%3D%3D)\n",
            "2. [Video 2](https://www.youtube.com/watch?v=krsBRQbOPQ4&pp=ygUHTXJCZWFzdA%3D%3D)\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here are the YouTube video links of the channel named MrBeast:\\n1. [Video 1](https://www.youtube.com/watch?v=jaRfBM7ESfc&pp=ygUHTXJCZWFzdA%3D%3D)\\n2. [Video 2](https://www.youtube.com/watch?v=krsBRQbOPQ4&pp=ygUHTXJCZWFzdA%3D%3D)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Can you tell me capital of Japan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "dMual5tGhLma",
        "outputId": "1f2af581-2377-4bd3-d92d-4bb3d9385c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Search` with `capital of Japan`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mTokyo\u001b[0m\u001b[32;1m\u001b[1;3mThe capital of Japan is Tokyo.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of Japan is Tokyo.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why we need memory?"
      ],
      "metadata": {
        "id": "UGrBszpEJ_t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_openai.invoke(\"Hi Open AI I am a software developer and currently living in Mumbai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRuIlbEoKDEb",
        "outputId": "3fb12e60-ee7b-4fdc-e08a-7c9cac0bb7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! It's great to meet another software developer. How can I assist you today?\")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_openai.invoke(\"In which city I am residing currently?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJkYrDiLKW2D",
        "outputId": "7a708421-d48d-4021-d24d-4d51e48f3ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, I do not have that information as I am an AI virtual assistant and do not have access to your personal details.\")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Buffer Memory"
      ],
      "metadata": {
        "id": "GEWREfqCLai2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "\n",
        "convo = ConversationChain(\n",
        "    llm=llm_openai,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ],
      "metadata": {
        "id": "EOJDUVj-LbtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(convo.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzqEuvddLxDY",
        "outputId": "a3b835be-f957-41df-aa27-f40faddd7971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.invoke(\"Hi Open AI I am a software developer and currently living in Mumbai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8bn2mwlL5T9",
        "outputId": "a8692708-e6b6-4751-a247-8bb13f0c7429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi Open AI I am a software developer and currently living in Mumbai\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Hi Open AI I am a software developer and currently living in Mumbai',\n",
              " 'history': '',\n",
              " 'response': \"Hello! Nice to meet you. That's great to hear that you're a software developer. Mumbai is such a vibrant city with a booming tech scene. How long have you been living there?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.invoke(\"In which city I am residing currently?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvecgV_IMOy8",
        "outputId": "ba0f655b-02f6-43f7-c706-41ce17e6d0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi Open AI I am a software developer and currently living in Mumbai\n",
            "AI: Hello! Nice to meet you. That's great to hear that you're a software developer. Mumbai is such a vibrant city with a booming tech scene. How long have you been living there?\n",
            "Human: In which city I am residing currently?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'In which city I am residing currently?',\n",
              " 'history': \"Human: Hi Open AI I am a software developer and currently living in Mumbai\\nAI: Hello! Nice to meet you. That's great to hear that you're a software developer. Mumbai is such a vibrant city with a booming tech scene. How long have you been living there?\",\n",
              " 'response': 'You mentioned earlier that you are currently living in Mumbai.'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Buffer Window"
      ],
      "metadata": {
        "id": "WoFZKKkNMro6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n",
        "convo_buffer_window = ConversationChain(\n",
        "    llm=llm_openai,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferWindowMemory(k=2)\n",
        ")"
      ],
      "metadata": {
        "id": "MfbAhZQoMzAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer_window.invoke(\"Hi Open AI I am a software developer and currently living in Mumbai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEcUjrISM8Ra",
        "outputId": "0796dcff-71fd-43b2-ac3e-a5318b1e5fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi Open AI I am a software developer and currently living in Mumbai\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Hi Open AI I am a software developer and currently living in Mumbai',\n",
              " 'history': '',\n",
              " 'response': \"Hello! It's nice to meet you, a software developer living in Mumbai. How can I assist you today?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer_window.invoke(\"I like to play badmintion\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj-cB_hdNF_A",
        "outputId": "045cde45-2220-4ec6-865a-f9e0ad4cb926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi Open AI I am a software developer and currently living in Mumbai\n",
            "AI: Hello! It's nice to meet you, a software developer living in Mumbai. How can I assist you today?\n",
            "Human: I like to play badmintion\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'I like to play badmintion',\n",
              " 'history': \"Human: Hi Open AI I am a software developer and currently living in Mumbai\\nAI: Hello! It's nice to meet you, a software developer living in Mumbai. How can I assist you today?\",\n",
              " 'response': \"That's great to hear! Badminton is a fun and exciting sport. Do you play casually with friends or do you participate in any organized leagues or tournaments in Mumbai?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer_window.invoke(\"I like to eat fries.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1jChOFCNLy9",
        "outputId": "fe99e7ec-8848-4318-a8c6-2ef7a2d04a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi Open AI I am a software developer and currently living in Mumbai\n",
            "AI: Hello! It's nice to meet you, a software developer living in Mumbai. How can I assist you today?\n",
            "Human: I like to play badmintion\n",
            "AI: That's great to hear! Badminton is a fun and exciting sport. Do you play casually with friends or do you participate in any organized leagues or tournaments in Mumbai?\n",
            "Human: I like to eat fries.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'I like to eat fries.',\n",
              " 'history': \"Human: Hi Open AI I am a software developer and currently living in Mumbai\\nAI: Hello! It's nice to meet you, a software developer living in Mumbai. How can I assist you today?\\nHuman: I like to play badmintion\\nAI: That's great to hear! Badminton is a fun and exciting sport. Do you play casually with friends or do you participate in any organized leagues or tournaments in Mumbai?\",\n",
              " 'response': 'Fries are a delicious snack! There are many places in Mumbai where you can find tasty fries. Do you have a favorite spot to get fries from, or do you prefer to make them at home?'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer_window.invoke(\"I like to watch cricket quite often in my spare time\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fVzqYX8NXsh",
        "outputId": "cf5b30ab-c8f6-4ac3-eea0-a57ea4781d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: I like to play badmintion\n",
            "AI: That's great to hear! Badminton is a fun and exciting sport. Do you play casually with friends or do you participate in any organized leagues or tournaments in Mumbai?\n",
            "Human: I like to eat fries.\n",
            "AI: Fries are a delicious snack! There are many places in Mumbai where you can find tasty fries. Do you have a favorite spot to get fries from, or do you prefer to make them at home?\n",
            "Human: I like to watch cricket quite often in my spare time\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'I like to watch cricket quite often in my spare time',\n",
              " 'history': \"Human: I like to play badmintion\\nAI: That's great to hear! Badminton is a fun and exciting sport. Do you play casually with friends or do you participate in any organized leagues or tournaments in Mumbai?\\nHuman: I like to eat fries.\\nAI: Fries are a delicious snack! There are many places in Mumbai where you can find tasty fries. Do you have a favorite spot to get fries from, or do you prefer to make them at home?\",\n",
              " 'response': \"Cricket is a popular sport in India, so I'm not surprised that you enjoy watching it in your spare time. There are so many exciting matches and tournaments to follow! Do you have a favorite team or player that you like to watch?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer_window.invoke(\"What i like to play?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7Tta6MANfvz",
        "outputId": "70bdf85a-3ae7-4889-9824-a503af72b55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: I like to eat fries.\n",
            "AI: Fries are a delicious snack! There are many places in Mumbai where you can find tasty fries. Do you have a favorite spot to get fries from, or do you prefer to make them at home?\n",
            "Human: I like to watch cricket quite often in my spare time\n",
            "AI: Cricket is a popular sport in India, so I'm not surprised that you enjoy watching it in your spare time. There are so many exciting matches and tournaments to follow! Do you have a favorite team or player that you like to watch?\n",
            "Human: What i like to play?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What i like to play?',\n",
              " 'history': \"Human: I like to eat fries.\\nAI: Fries are a delicious snack! There are many places in Mumbai where you can find tasty fries. Do you have a favorite spot to get fries from, or do you prefer to make them at home?\\nHuman: I like to watch cricket quite often in my spare time\\nAI: Cricket is a popular sport in India, so I'm not surprised that you enjoy watching it in your spare time. There are so many exciting matches and tournaments to follow! Do you have a favorite team or player that you like to watch?\",\n",
              " 'response': \"I'm not sure what you like to play. Could you provide more details or context so I can give you a more accurate response?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Knowledge Graph"
      ],
      "metadata": {
        "id": "nNQ8XqdqOpKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationKGMemory"
      ],
      "metadata": {
        "id": "yqeNRe8eP_U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kg_template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n",
        "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
        "\n",
        "Relevant Information:\n",
        "\n",
        "{history}\n",
        "\n",
        "Conversation:\n",
        "Human: {input}\n",
        "AI:\"\"\""
      ],
      "metadata": {
        "id": "IWW2zzVpOsOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kg_prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=kg_template)\n",
        "conversation_with_kg = ConversationChain(\n",
        "    llm=llm_openai, verbose=True, prompt=kg_prompt, memory=ConversationKGMemory(llm=llm_openai)\n",
        ")"
      ],
      "metadata": {
        "id": "MprsJNqiP3NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_kg.invoke(\"Hi, what's up?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU2VU2I1QCn2",
        "outputId": "a41699e9-8b6b-4bf5-af32-3fc7523b2390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
            "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
            "\n",
            "Relevant Information:\n",
            "\n",
            "\n",
            "\n",
            "Conversation:\n",
            "Human: Hi, what's up?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"Hi, what's up?\",\n",
              " 'history': '',\n",
              " 'response': \"Hello! I'm doing well, thank you for asking. I am currently processing information and ready to assist you with any questions you may have. How can I help you today?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_kg.invoke(\n",
        "    \"My name is James and I'm helping Will. He's an engineer.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JahcihtQLSC",
        "outputId": "0d8f22b8-3da2-4e0f-8fe7-d9605d1e0536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
            "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
            "\n",
            "Relevant Information:\n",
            "\n",
            "\n",
            "\n",
            "Conversation:\n",
            "Human: My name is James and I'm helping Will. He's an engineer.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"My name is James and I'm helping Will. He's an engineer.\",\n",
              " 'history': '',\n",
              " 'response': \"Hello James! It's nice to meet you. How can I assist you with helping Will, the engineer?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_kg.invoke(\"What do you know about Will?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MEYywQNQWkg",
        "outputId": "a58992a2-dfeb-4573-a36f-412659e5f2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
            "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
            "\n",
            "Relevant Information:\n",
            "\n",
            "On Will: Will is engineer.\n",
            "\n",
            "Conversation:\n",
            "Human: What do you know about Will?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What do you know about Will?',\n",
              " 'history': 'On Will: Will is engineer.',\n",
              " 'response': 'Will is an engineer.'}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}